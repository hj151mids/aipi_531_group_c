# Improving GRU4Rec with the Addition of Feature Network
Author: Ya-Yun Huang, Haoliang Jiang, Yufei Lei, Chloe Liu
## 00 Summary
This project focuses on improving the GRU4Rec + SNQN recommender with the help of taking user or item features into account. We used two different offline evaluation metrics (NDCG and HR) for benchmarking. Our findings suggest that the inclusion of a feature network can improve the results of product recommendation. 
## 01 Introduction
## 02 Literature Review
Reinforcement Learning is a type of machine learning that allows an agent to learn how to behave in an environment by trial and error. In the context of recommender systems, RL can be used to learn how to recommend items to users in a way that maximizes their satisfaction.  

One of the challenges of applying RL to recommender systems is that the 3environment is often very large and complex. This makes it difficult to train an RL agent to learn the optimal policy. Another challenge is that the reward signal is often delayed, which can make it difficult for the agent to learn the correct association between its actions and rewards it receives.  

To address these challenges, Xin Xin et al. have proposed a number of methods for combining RL with supervised learning. One such method is called supervised negative Q-learning (SNQN). SNQN works by first training a supervised learning model to predict the reward for each possible action. The RL agent then uses this model to estimate the Q-values for each action. The Q-values are then used to train the RL agent to learn the optimal policy. In their paper, the experimental results have shown that SNQN can achieve significantly better performance than supervised learning methods. This is because SNQN is able to learn the optimal policy more quickly and efficiently. It is also able to generalize better to new users and items than other methods.  

In our project, we would like to make improvements on the aforementioned SNQN model by including a feature network such that the recommender is more personalized.  
Personalized real-time recommendation has had a profound impact on retail, media, entertainment, and other industries. However, developing recommender systems for every use case is costly, time-consuming, and resource-intensive. To fill this gap, Yifei Ma et al. have proposed a number of black-box recommender systems that can adapt to a diverse set of scenarios without the need for manual tuning. The structure that Yifei proposed allowed inclusion of item features and cold start. The inclusion of feature items is implemented by combining the hidden state generated by the HRNN model with the output of a feature network whose inputs are one-hot encoded features. The loss is adjusted by assigning weights to each item when summing up. We will adopt this methodology and apply it to modify the backbone of Xin Xin's SNQN model.
## 03 Methodology
### 031 Data Cleaning and Feature Engineering
For the Retail Rocket dataset, we largely followed the same data cleaning processes as that proposed by Xin Xin's paper. We first removed the users or items that have lower than or equal to 2 interactions. We then label encoded the session_ids and behavior. Once we created a sorted data table, we read in the two item properties files and cancated these 2 files. Items without category_ids were removed. We named the label-encoded dataset event_with_prop. Last but not least, we one-hot encoded the merged dataset of event plus item features. We one-hot encoded the categoryid and made a dataframe with each item's corresponding one-hot encoded category id and named the table item_category.  

Data processing is done slightly different from that conducted on the Retail Rocket dataset, however, the model outcome should not be largely affected. We first sampled 3000 items and users to generate a "not buy" dataset. We then merged this generated negative feedback dataset with the positive feedback dataset and form one single master data. We renamed some columns and cleaned the timestamp column by redefining the date format. We then follow the same steps as those for the Retail Rocket dataset. We removed users with less than or equal to 2 interactions. We then label encoded the item_id and categoryid. Last but not least, we one-hot encoded the categoryid and made a dataframe with each item's corresponding one-hot-encoded category id and named the table item_category.  

We used the source code from Xinxin where they implemented the Q Network. We added a dense layer called `output_phi` that takes into account the phi score of the previous non feature based network, whose input size is the dimension of hidden states generated by the pervious `output1` layer and whose output size is the item_num. Then the `feature_embeddings` 
## 04 Results
## 05 Conclusion and Limitations
## 06 Instructions
1. We have created notebooks for you to run our models in Google Colab. You will need to upload this repository, along with the H&M and Retail Rocket datasets to Google Colab. Please refer to the following table for the:  

| Model | Model File Path| Driver Notebook File Path | Data Source |
|:--------------|:-------------|:-------------|:-------------
| GRU4Rec + Feature Network, SNQN On| ~./HM_Chloe/SNQN_v1.py | ~./AIPI531_Project_SNQN_itemfeatures_retailrocket.ipynb | Retail Rocket |
| GRU4Rec, SNQN On| **To be updated** | ~./AIPI531_Project_SNQN_retailrocket.ipynb | Retail Rocket |
| GRU4Rec, SNQN Off (Experiment) | ~./Kaggle/SNQN_v3.py | ~./Kaggle/GRU4REC.ipynb | Retail Rocket |
| GRU4Rec + Feature Network, SNQN On | ~./HM_Chloe/SNQN_v1.py | ~./HM_Chloe/AIPI531_Project_SNQN_itemfeatures_HM.ipynb | H&M |
| GRU4Rec, SNQN On | ~./HM_Chloe/SNQN.py | ~./HM_Chloe/AIPI531_Project_SNQN_HM.ipynb | H&M |


2. Open up terminal in the selected driver notebook. Here is an example photo of how you can access it: ![image](https://user-images.githubusercontent.com/90075179/236083065-e42fad95-ac6f-4b74-9651-c3de9728d20a.png)  

3. To install the requirements in a requirements.txt file, you can use the following command: `pip install -r requirements.txt`. This will install all of the packages listed in the requirements.txt file, along with their dependencies  

4. If you are using a virtual environment, you will need to activate it before installing the requirements: type `source venv/bin/activate` and then `pip install -r requirements.txt`  

5. Once the requirements are installed, you can start using them in the project. If you have never subscribed to Colab Pro, then you will not have access to the terminal. In this case, simply copy the version numbers and paste it to the `!pip install...` line in the driver notebook  

6. 

## 07 Reference
1. Supervised Advantage Actor-Critic for Recommender Systems | X. Xin, A. Karatzoglou, I. Arapakis, and J. M. Jose | Proceedings of ACM Conference (Conferenceâ€™17), 2021.  
2. Temporal-Contextual Recommendation in Real-Time | Y. Ma, H. Lin, B. Narayanaswamy, H. Ding | The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2020.
